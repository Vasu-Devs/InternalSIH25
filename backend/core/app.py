# app.py - College Assistant RAG Backend (SIH + Gemini Flash + Google Voice, Full Version)

from dotenv import load_dotenv
load_dotenv()
import json
import os
import time
import re
from pathlib import Path
from typing import List, Optional, Dict, Any
from concurrent.futures import ThreadPoolExecutor
import tempfile
import asyncio
import warnings

from fastapi import FastAPI, HTTPException, Body, UploadFile, File, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, FileResponse
from pydantic import BaseModel
import uvicorn

# LangChain (for RAG vectorstore)
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings

# Google AI SDKs
import google.generativeai as genai


# PDF loaders
try:
    import pdfplumber
    PDFPLUMBER_AVAILABLE = True
except ImportError:
    PDFPLUMBER_AVAILABLE = False

try:
    import PyPDF2
    PYPDF2_AVAILABLE = True
except ImportError:
    PYPDF2_AVAILABLE = False

try:
    from langchain_community.document_loaders import PyPDFLoader
    LANGCHAIN_PDF_AVAILABLE = True
except ImportError:
    LANGCHAIN_PDF_AVAILABLE = False

warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", module="langchain")

# -------------------- CONFIG --------------------
PERSIST_DIR = os.getenv("PERSIST_DIR", "./chroma_db")
DEFAULT_K = int(os.getenv("DEFAULT_K", "5"))
CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", "800"))
CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", "100"))
TEMP_MD_DIR = os.getenv("TEMP_MD_DIR", "./temp_md")
BATCH_SIZE = int(os.getenv("BATCH_SIZE", "20"))

os.makedirs(PERSIST_DIR, exist_ok=True)
os.makedirs(TEMP_MD_DIR, exist_ok=True)

executor = ThreadPoolExecutor(max_workers=4)

# Gemini + Google Clients
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
GEMINI_MODEL = "gemini-2.0-flash"

# -------------------- FASTAPI --------------------
app = FastAPI(
    title="College Assistant RAG API (SIH + Gemini + Google Voice)",
    version="8.0",
    description="RAG backend with admin approval, student chat, streaming chat, and Google voice chat"
)
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:5173",
        "http://127.0.0.1:5173",
        "http://localhost:3000",
        "http://127.0.0.1:3000"
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# -------------------- MODELS --------------------
class ChatRequest(BaseModel):
    message: str
    department: Optional[str] = "General"
    k: Optional[int] = DEFAULT_K

class ChatResponse(BaseModel):
    response: str
    department: str
    sources: List[Dict[str, Any]] = []
    elapsed_seconds: float

class UploadResponse(BaseModel):
    message: str
    num_chunks: int
    stored_at: str

# -------------------- GLOBALS --------------------
_vectorstore = None
upload_progress = {}
_doc_approval_status: Dict[str, bool] = {}

# -------------------- UTILS --------------------
def extract_text_from_pdf(pdf_path: Path) -> str:
    if PDFPLUMBER_AVAILABLE:
        with pdfplumber.open(pdf_path) as pdf:
            return "\n\n".join([p.extract_text() or "" for p in pdf.pages])
    if LANGCHAIN_PDF_AVAILABLE:
        loader = PyPDFLoader(str(pdf_path))
        return "\n\n".join([p.page_content for p in loader.load()])
    if PYPDF2_AVAILABLE:
        reader = PyPDF2.PdfReader(open(pdf_path, "rb"))
        return "\n\n".join([p.extract_text() or "" for p in reader.pages])
    raise ImportError("No PDF loader available. Install pdfplumber or PyPDF2.")

def preprocess_text(text: str) -> str:
    text = text.replace("&amp;", "&")
    text = re.sub(r"\n{2,}", "\n", text)
    return text.strip()

def chunk_text(text: str, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP) -> List[str]:
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=overlap,
        separators=["\n\n", "\n", ". ", "! ", "? ", " "],
        length_function=len,
    )
    chunks = splitter.split_text(text)
    return [c.strip() for c in chunks if len(c.strip()) > 50]

def embed_and_store_fast(chunks: List[str], metadata: Optional[List[Dict]] = None, batch_size: int = BATCH_SIZE):
    global _vectorstore
    if _vectorstore is None:
        embeddings = OllamaEmbeddings(model="nomic-embed-text")
        _vectorstore = Chroma(persist_directory=PERSIST_DIR, embedding_function=embeddings)

    total = len(chunks)
    if total <= batch_size:
        _vectorstore.add_texts(texts=chunks, metadatas=metadata)
        return _vectorstore

    futures = []
    for i in range(0, total, batch_size):
        batch = chunks[i:i+batch_size]
        meta_batch = metadata[i:i+batch_size] if metadata else None
        futures.append(executor.submit(_vectorstore.add_texts, texts=batch, metadatas=meta_batch))

    for f in futures:
        f.result()
    return _vectorstore

def create_college_context_prompt(query: str, department: str) -> str:
    return f"""
    You are a knowledgeable and authoritative college assistant specializing in {department}.
    Your job is to answer student questions about college rules, regulations, and policies.

    IMPORTANT:
    - Always give a clear, direct, and complete answer.
    - Never respond with "tell me more" or "which library"‚Äîassume the context is THIS COLLEGE.
    - Base your answers on the college‚Äôs policies and common academic practices.
    - Respond in the same language as the student's last question.

    Student Question: {query}
    """



async def run_gemini(query: str) -> str:
    model = genai.GenerativeModel(GEMINI_MODEL)
    resp = model.generate_content(query)
    return resp.text if resp and resp.text else "No response generated."

# -------------------- GLOBALS --------------------
_vectorstore = None
upload_progress = {}
_doc_approval_status: Dict[str, bool] = {}
_chat_history: Dict[str, List[Dict[str, str]]] = {}
  # user_id -> [{"role": "user/assistant", "content": "..."}]


# -------------------- STARTUP --------------------
@app.on_event("startup")
def startup_event():
    global _vectorstore
    try:
        embeddings = OllamaEmbeddings(model="nomic-embed-text")
        _vectorstore = Chroma(persist_directory=PERSIST_DIR, embedding_function=embeddings)
        print(f"‚úÖ Vector DB loaded from {PERSIST_DIR}")
    except Exception as e:
        print(f"‚ùå Failed to load vectorstore: {e}")
        _vectorstore = None
    print("üî• Gemini + Google Voice backend ready!")

# -------------------- HEALTH --------------------
@app.get("/health")
def health():
    return {"status": "ok", "vectorstore_loaded": _vectorstore is not None}

# -------------------- STUDENT CHAT --------------------
@app.post("/chat", response_model=ChatResponse)
async def chat_with_assistant(payload: ChatRequest = Body(...), user_id: str = "default"):
    if _vectorstore is None:
        return ChatResponse(
            response="Vectorstore not initialized. Upload some PDFs first or call /test_retrieval.",
            department=payload.department,
            sources=[],
            elapsed_seconds=0.0,
        )

    # init history for this user
    if user_id not in _chat_history:
        _chat_history[user_id] = []

    # add current user query to history
    _chat_history[user_id].append({"role": "student", "content": payload.message})

    # build conversation text
    conversation = "\n".join(
        [f"{msg['role'].capitalize()}: {msg['content']}" for msg in _chat_history[user_id]]
    )

    prompt = f"""
    The following is a conversation between a student and a helpful college assistant specializing in {payload.department}.
    You must maintain context across turns.
    Always respond in the same language as the student's last question.
    Always assume the student is asking about THIS college.

    Conversation so far:
    {conversation}

    Assistant:
    """

    start = time.time()
    result_text = await run_gemini(prompt)

    # save assistant reply to history
    _chat_history[user_id].append({"role": "assistant", "content": result_text})

    # keep history manageable (last 10 exchanges)
    _chat_history[user_id] = _chat_history[user_id][-20:]

    # retrieve sources
    retriever = _vectorstore.as_retriever(search_kwargs={"k": payload.k or DEFAULT_K})
    docs = retriever.get_relevant_documents(payload.message)
    approved = [d for d in docs if _doc_approval_status.get(d.metadata.get("source"), False)]

    elapsed = time.time() - start
    return ChatResponse(
        response=result_text,
        department=payload.department,
        sources=[{"source": d.metadata.get("source")} for d in approved],
        elapsed_seconds=round(elapsed, 3)
    )


@app.post("/chat_stream")
async def chat_stream(payload: ChatRequest = Body(...)):
    query = create_college_context_prompt(payload.message, payload.department)

    def event_stream():
        retriever = _vectorstore.as_retriever(search_kwargs={"k": payload.k or DEFAULT_K})
        docs = retriever.get_relevant_documents(payload.message)
        yield json.dumps({"type": "status", "message": "üìö searching documents"}) + "\n"
        for d in docs:
            preview = d.page_content[:120].replace("\n", " ")
            yield json.dumps({"type": "doc", "source": d.metadata.get("source"), "preview": preview}) + "\n"

        yield json.dumps({"type": "status", "message": "ü§î Generating answer..."}) + "\n"

        model = genai.GenerativeModel(GEMINI_MODEL)
        response = model.generate_content(query, stream=True)
        for chunk in response:
            if chunk.text:
                yield json.dumps({"type": "token", "text": chunk.text}) + "\n"

        yield json.dumps({"type": "done"}) + "\n"

    return StreamingResponse(event_stream(), media_type="application/json")

# -------------------- ADMIN CHAT --------------------
@app.post("/admin_chat", response_model=ChatResponse)
async def admin_chat(payload: ChatRequest = Body(...)):
    start = time.time()
    query = create_college_context_prompt(payload.message, payload.department)
    result_text = await run_gemini(query)

    retriever = _vectorstore.as_retriever(search_kwargs={"k": payload.k or DEFAULT_K})
    docs = retriever.get_relevant_documents(payload.message)

    elapsed = time.time() - start
    return ChatResponse(
        response=result_text,
        department=payload.department,
        sources=[{"source": d.metadata.get("source")} for d in docs],
        elapsed_seconds=round(elapsed, 3)
    )

# -------------------- ADMIN DOC CONTROL --------------------
@app.post("/upload_pdf", response_model=UploadResponse)
async def upload_pdf(file: UploadFile = File(...)):
    if not file.filename.lower().endswith(".pdf"):
        raise HTTPException(status_code=400, detail="Only PDFs supported")
    content = await file.read()
    temp_path = Path(TEMP_MD_DIR) / file.filename
    temp_path.write_bytes(content)

    text = extract_text_from_pdf(temp_path)
    text = preprocess_text(text)
    chunks = chunk_text(text)
    if not chunks:
        raise HTTPException(status_code=400, detail="No valid text extracted")

    metadata = [{"source": file.filename, "chunk_id": i} for i in range(len(chunks))]
    embed_and_store_fast(chunks, metadata)
    _doc_approval_status[file.filename] = False

    try: temp_path.unlink()
    except: pass

    return UploadResponse(message="Upload successful (awaiting approval)", num_chunks=len(chunks), stored_at=PERSIST_DIR)

async def process_pdf_background(upload_id: str, file_content: bytes, filename: str):
    try:
        temp_path = Path(TEMP_MD_DIR) / filename
        temp_path.write_bytes(file_content)
        upload_progress[upload_id] = {"status": "processing", "progress": 20}

        text = extract_text_from_pdf(temp_path)
        text = preprocess_text(text)
        chunks = chunk_text(text)
        if not chunks:
            upload_progress[upload_id] = {"status": "error", "error": "No valid text"}
            return

        metadata = [{"source": filename, "chunk_id": i} for i in range(len(chunks))]
        embed_and_store_fast(chunks, metadata)
        _doc_approval_status[filename] = False

        upload_progress[upload_id] = {"status": "completed", "progress": 100, "num_chunks": len(chunks)}
        try: temp_path.unlink()
        except: pass
    except Exception as e:
        upload_progress[upload_id] = {"status": "error", "error": str(e)}

@app.post("/upload_pdf_async")
async def upload_pdf_async(background_tasks: BackgroundTasks, file: UploadFile = File(...)):
    content = await file.read()
    upload_id = f"{file.filename}_{int(time.time())}"
    upload_progress[upload_id] = {"status": "started", "progress": 0}
    background_tasks.add_task(process_pdf_background, upload_id, content, file.filename)
    return {"upload_id": upload_id, "message": "Upload started"}

@app.get("/upload_status/{upload_id}")
def get_upload_status(upload_id: str):
    return upload_progress.get(upload_id, {"status": "not_found"})

@app.post("/approve_doc/{filename}")
def approve_doc(filename: str):
    if filename not in _doc_approval_status:
        raise HTTPException(status_code=404, detail="Doc not found")
    _doc_approval_status[filename] = True
    return {"message": f"{filename} approved and now available to students"}

@app.delete("/delete_doc/{filename}")
def delete_doc(filename: str):
    global _vectorstore
    try:
        _vectorstore._collection.delete(where={"source": filename})
        _doc_approval_status.pop(filename, None)
        return {"message": f"{filename} deleted from vectorstore"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Delete failed: {e}")

@app.post("/test_retrieval")
def test_retrieval(request: ChatRequest):
    if _vectorstore is None:
        return {"status": "error", "message": "No docs uploaded"}
    retriever = _vectorstore.as_retriever(search_kwargs={"k": 5})
    docs = retriever.get_relevant_documents(request.message)
    return {
        "status": "success",
        "query": request.message,
        "retrieved_docs": [
            {"source": d.metadata.get("source"), "preview": d.page_content[:200]}
            for d in docs
        ]
    }

# -------------------- VOICE CHAT --------------------
# @app.post("/voice_chat")
# async def voice_chat(file: UploadFile = File(...)):
#     audio_path = Path(tempfile.mktemp(suffix=".wav"))
#     audio_path.write_bytes(await file.read())

#     # Speech-to-Text (Google)
#     with open(audio_path, "rb") as f:
#         audio = speech.RecognitionAudio(content=f.read())

#     config = speech.RecognitionConfig(
#         encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
#         sample_rate_hertz=16000,
#         language_code="pa-IN",  # can change dynamically
#     )

#     response = speech_client.recognize(config=config, audio=audio)
#     query = " ".join([r.alternatives[0].transcript for r in response.results])

#     if not query.strip():
#         return {"error": "No speech recognized"}

#     # RAG + Gemini
#     result_text = await run_gemini(query)

#     # TTS response (Google)
#     synthesis_input = texttospeech.SynthesisInput(text=result_text)
#     voice = texttospeech.VoiceSelectionParams(
#         language_code="pa-IN", ssml_gender=texttospeech.SsmlVoiceGender.NEUTRAL
#     )
#     audio_config = texttospeech.AudioConfig(audio_encoding=texttospeech.AudioEncoding.MP3)

#     tts_response = tts_client.synthesize_speech(
#         input=synthesis_input, voice=voice, audio_config=audio_config
#     )

#     out_path = Path(tempfile.mktemp(suffix=".mp3"))
#     out_path.write_bytes(tts_response.audio_content)

#     try: audio_path.unlink()
#     except: pass

#     return FileResponse(out_path, media_type="audio/mpeg", filename="response.mp3")

# -------------------- RUN --------------------
if __name__ == "__main__":
    uvicorn.run("app:app", host="127.0.0.1", port=8000, reload=True)
